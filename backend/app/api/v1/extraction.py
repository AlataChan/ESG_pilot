"""
ä¿¡æ¯æå–API - æ–‡æ¡£åˆ†æã€å…³é”®ä¿¡æ¯æå–å’Œæ‘˜è¦ç”Ÿæˆæ¥å£
æ”¯æŒæ™ºèƒ½æ–‡æ¡£åˆ†æå’Œç»“æ„åŒ–ä¿¡æ¯è¾“å‡º
"""

from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from typing import List, Optional, Dict, Any
import logging
from datetime import datetime

from app.services.extraction_service import get_extraction_service, InformationExtractionService
from app.core.response import APIResponse, create_response
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/extraction", tags=["Information Extraction"])


# ========== è¯·æ±‚/å“åº”æ¨¡å‹ ==========

class ExtractionRequest(BaseModel):
    """ä¿¡æ¯æå–è¯·æ±‚æ¨¡å‹"""
    document_id: str = Field(..., description="æ–‡æ¡£ID")
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    extraction_types: List[str] = Field(
        ["summary", "entities", "keywords", "key_info"], 
        description="æå–ç±»å‹åˆ—è¡¨"
    )


class BatchExtractionRequest(BaseModel):
    """æ‰¹é‡ä¿¡æ¯æå–è¯·æ±‚æ¨¡å‹"""
    document_ids: List[str] = Field(..., description="æ–‡æ¡£IDåˆ—è¡¨", max_items=10)
    user_id: str = Field(..., description="ç”¨æˆ·ID")
    extraction_types: List[str] = Field(
        ["summary", "entities", "keywords"], 
        description="æå–ç±»å‹åˆ—è¡¨"
    )


class DocumentSummaryResponse(BaseModel):
    """æ–‡æ¡£æ‘˜è¦å“åº”æ¨¡å‹"""
    title: str = Field(..., description="æ–‡æ¡£æ ‡é¢˜")
    brief_summary: str = Field(..., description="ç®€è¦æ‘˜è¦")
    detailed_summary: str = Field(..., description="è¯¦ç»†æ‘˜è¦")
    key_points: List[str] = Field(..., description="å…³é”®è¦ç‚¹")
    structure_summary: str = Field(..., description="ç»“æ„æ€§æ‘˜è¦")
    confidence: float = Field(..., description="æ‘˜è¦è´¨é‡ç½®ä¿¡åº¦", ge=0, le=1)


class ExtractedEntityResponse(BaseModel):
    """æå–å®ä½“å“åº”æ¨¡å‹"""
    text: str = Field(..., description="å®ä½“æ–‡æœ¬")
    type: str = Field(..., description="å®ä½“ç±»å‹")
    confidence: float = Field(..., description="ç½®ä¿¡åº¦", ge=0, le=1)
    context: str = Field(..., description="ä¸Šä¸‹æ–‡")
    position: int = Field(..., description="åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®")


class KeyInformationResponse(BaseModel):
    """å…³é”®ä¿¡æ¯å“åº”æ¨¡å‹"""
    content: str = Field(..., description="ä¿¡æ¯å†…å®¹")
    importance: float = Field(..., description="é‡è¦æ€§è¯„åˆ†", ge=0, le=1)
    category: str = Field(..., description="ä¿¡æ¯ç±»åˆ«")
    keywords: List[str] = Field(..., description="å…³é”®è¯")
    source_section: str = Field(..., description="æ¥æºç« èŠ‚")


class ExtractionResultResponse(BaseModel):
    """ä¿¡æ¯æå–ç»“æœå“åº”æ¨¡å‹"""
    document_id: str = Field(..., description="æ–‡æ¡£ID")
    document_name: str = Field(..., description="æ–‡æ¡£åç§°")
    summary: DocumentSummaryResponse = Field(..., description="æ–‡æ¡£æ‘˜è¦")
    key_information: List[KeyInformationResponse] = Field(..., description="å…³é”®ä¿¡æ¯")
    entities: List[ExtractedEntityResponse] = Field(..., description="æå–çš„å®ä½“")
    tags: List[str] = Field(..., description="æ–‡æ¡£æ ‡ç­¾")
    word_count: int = Field(..., description="å­—æ•°ç»Ÿè®¡")
    paragraph_count: int = Field(..., description="æ®µè½æ•°é‡")
    section_count: int = Field(..., description="ç« èŠ‚æ•°é‡")
    extraction_timestamp: str = Field(..., description="æå–æ—¶é—´")
    processing_time: float = Field(..., description="å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰")


# ========== APIæ¥å£ ==========

@router.post("/analyze", response_model=APIResponse[ExtractionResultResponse])
async def analyze_document(
    request: ExtractionRequest,
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    åˆ†æå•ä¸ªæ–‡æ¡£å¹¶æå–ä¿¡æ¯
    
    å¯¹æŒ‡å®šæ–‡æ¡£è¿›è¡Œå…¨é¢åˆ†æï¼ŒåŒ…æ‹¬ï¼š
    - ç”Ÿæˆå¤šå±‚æ¬¡æ‘˜è¦
    - æå–å…³é”®ä¿¡æ¯ç‚¹
    - è¯†åˆ«é‡è¦å®ä½“
    - ç”Ÿæˆæ™ºèƒ½æ ‡ç­¾
    """
    try:
        logger.info(f"ğŸ“Š Starting document analysis: {request.document_id} (user: {request.user_id})")
        
        # æ‰§è¡Œä¿¡æ¯æå–
        extraction_result = await extraction_service.extract_information(
            document_id=request.document_id,
            user_id=request.user_id
        )
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        response = ExtractionResultResponse(
            document_id=extraction_result.document_id,
            document_name=extraction_result.document_name,
            summary=DocumentSummaryResponse(
                title=extraction_result.summary.title,
                brief_summary=extraction_result.summary.brief_summary,
                detailed_summary=extraction_result.summary.detailed_summary,
                key_points=extraction_result.summary.key_points,
                structure_summary=extraction_result.summary.structure_summary,
                confidence=extraction_result.summary.confidence
            ),
            key_information=[
                KeyInformationResponse(
                    content=info.content,
                    importance=info.importance,
                    category=info.category,
                    keywords=info.keywords,
                    source_section=info.source_section
                )
                for info in extraction_result.key_information
            ],
            entities=[
                ExtractedEntityResponse(
                    text=entity.text,
                    type=entity.type,
                    confidence=entity.confidence,
                    context=entity.context,
                    position=entity.position
                )
                for entity in extraction_result.entities
            ],
            tags=extraction_result.tags,
            word_count=extraction_result.word_count,
            paragraph_count=extraction_result.paragraph_count,
            section_count=extraction_result.section_count,
            extraction_timestamp=extraction_result.extraction_timestamp.isoformat(),
            processing_time=extraction_result.processing_time
        )
        
        logger.info(f"âœ… Document analysis completed in {extraction_result.processing_time:.2f}s")
        return create_response(response)
        
    except Exception as e:
        logger.error(f"âŒ Document analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£åˆ†æå¤±è´¥: {str(e)}")


@router.get("/summary/{document_id}", response_model=APIResponse[DocumentSummaryResponse])
async def get_document_summary(
    document_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    è·å–æ–‡æ¡£æ‘˜è¦
    
    å¿«é€Ÿç”Ÿæˆæ–‡æ¡£çš„å¤šå±‚æ¬¡æ‘˜è¦ï¼ŒåŒ…æ‹¬ç®€è¦æ‘˜è¦ã€è¯¦ç»†æ‘˜è¦å’Œå…³é”®è¦ç‚¹ã€‚
    """
    try:
        logger.info(f"ğŸ“ Generating summary for document: {document_id}")
        
        # æ‰§è¡Œä¿¡æ¯æå–ï¼ˆä»…æ‘˜è¦éƒ¨åˆ†ï¼‰
        extraction_result = await extraction_service.extract_information(
            document_id=document_id,
            user_id=user_id
        )
        
        # è¿”å›æ‘˜è¦ä¿¡æ¯
        summary_response = DocumentSummaryResponse(
            title=extraction_result.summary.title,
            brief_summary=extraction_result.summary.brief_summary,
            detailed_summary=extraction_result.summary.detailed_summary,
            key_points=extraction_result.summary.key_points,
            structure_summary=extraction_result.summary.structure_summary,
            confidence=extraction_result.summary.confidence
        )
        
        logger.info(f"âœ… Summary generated (confidence: {extraction_result.summary.confidence:.2%})")
        return create_response(summary_response)
        
    except Exception as e:
        logger.error(f"âŒ Summary generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.get("/entities/{document_id}", response_model=APIResponse[List[ExtractedEntityResponse]])
async def get_document_entities(
    document_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    entity_type: Optional[str] = Query(None, description="å®ä½“ç±»å‹è¿‡æ»¤"),
    min_confidence: float = Query(0.5, description="æœ€å°ç½®ä¿¡åº¦", ge=0, le=1),
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    è·å–æ–‡æ¡£å®ä½“
    
    æå–æ–‡æ¡£ä¸­çš„é‡è¦å®ä½“ï¼Œå¦‚ç»„ç»‡æœºæ„ã€äººå‘˜èŒä½ã€è´¢åŠ¡æ•°æ®ç­‰ã€‚
    """
    try:
        logger.info(f"ğŸ·ï¸ Extracting entities for document: {document_id}")
        
        # æ‰§è¡Œä¿¡æ¯æå–
        extraction_result = await extraction_service.extract_information(
            document_id=document_id,
            user_id=user_id
        )
        
        # è¿‡æ»¤å®ä½“
        entities = extraction_result.entities
        
        if entity_type:
            entities = [e for e in entities if e.type == entity_type]
        
        if min_confidence > 0:
            entities = [e for e in entities if e.confidence >= min_confidence]
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        entity_responses = [
            ExtractedEntityResponse(
                text=entity.text,
                type=entity.type,
                confidence=entity.confidence,
                context=entity.context,
                position=entity.position
            )
            for entity in entities
        ]
        
        logger.info(f"âœ… Extracted {len(entity_responses)} entities")
        return create_response(entity_responses)
        
    except Exception as e:
        logger.error(f"âŒ Entity extraction failed: {e}")
        raise HTTPException(status_code=500, detail=f"å®ä½“æå–å¤±è´¥: {str(e)}")


@router.get("/key-information/{document_id}", response_model=APIResponse[List[KeyInformationResponse]])
async def get_key_information(
    document_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    category: Optional[str] = Query(None, description="ä¿¡æ¯ç±»åˆ«è¿‡æ»¤"),
    min_importance: float = Query(0.5, description="æœ€å°é‡è¦æ€§", ge=0, le=1),
    limit: int = Query(10, description="è¿”å›æ•°é‡é™åˆ¶", ge=1, le=50),
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    è·å–å…³é”®ä¿¡æ¯
    
    æå–æ–‡æ¡£ä¸­çš„å…³é”®ä¿¡æ¯ç‚¹ï¼ŒæŒ‰é‡è¦æ€§æ’åºã€‚
    """
    try:
        logger.info(f"ğŸ”‘ Extracting key information for document: {document_id}")
        
        # æ‰§è¡Œä¿¡æ¯æå–
        extraction_result = await extraction_service.extract_information(
            document_id=document_id,
            user_id=user_id
        )
        
        # è¿‡æ»¤å…³é”®ä¿¡æ¯
        key_info = extraction_result.key_information
        
        if category:
            key_info = [info for info in key_info if info.category == category]
        
        if min_importance > 0:
            key_info = [info for info in key_info if info.importance >= min_importance]
        
        # é™åˆ¶æ•°é‡
        key_info = key_info[:limit]
        
        # è½¬æ¢ä¸ºå“åº”æ ¼å¼
        key_info_responses = [
            KeyInformationResponse(
                content=info.content,
                importance=info.importance,
                category=info.category,
                keywords=info.keywords,
                source_section=info.source_section
            )
            for info in key_info
        ]
        
        logger.info(f"âœ… Extracted {len(key_info_responses)} key information points")
        return create_response(key_info_responses)
        
    except Exception as e:
        logger.error(f"âŒ Key information extraction failed: {e}")
        raise HTTPException(status_code=500, detail=f"å…³é”®ä¿¡æ¯æå–å¤±è´¥: {str(e)}")


@router.get("/tags/{document_id}")
async def get_document_tags(
    document_id: str,
    user_id: str = Query(..., description="ç”¨æˆ·ID"),
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    è·å–æ–‡æ¡£æ ‡ç­¾
    
    åŸºäºæ–‡æ¡£å†…å®¹è‡ªåŠ¨ç”Ÿæˆæ™ºèƒ½æ ‡ç­¾ã€‚
    """
    try:
        logger.info(f"ğŸ·ï¸ Generating tags for document: {document_id}")
        
        # æ‰§è¡Œä¿¡æ¯æå–
        extraction_result = await extraction_service.extract_information(
            document_id=document_id,
            user_id=user_id
        )
        
        result = {
            "document_id": document_id,
            "tags": extraction_result.tags,
            "tag_count": len(extraction_result.tags),
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Generated {len(extraction_result.tags)} tags")
        return create_response(result)
        
    except Exception as e:
        logger.error(f"âŒ Tag generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {str(e)}")


@router.post("/batch-analyze")
async def batch_analyze_documents(
    request: BatchExtractionRequest,
    background_tasks: BackgroundTasks,
    extraction_service: InformationExtractionService = Depends(get_extraction_service)
):
    """
    æ‰¹é‡åˆ†ææ–‡æ¡£
    
    å¯¹å¤šä¸ªæ–‡æ¡£è¿›è¡Œæ‰¹é‡ä¿¡æ¯æå–ï¼Œé€‚ç”¨äºå¤§é‡æ–‡æ¡£çš„æ‰¹å¤„ç†åœºæ™¯ã€‚
    """
    try:
        logger.info(f"ğŸ“Š Starting batch analysis for {len(request.document_ids)} documents")
        
        # åˆ›å»ºæ‰¹å¤„ç†ä»»åŠ¡
        task_id = f"batch_{datetime.now().timestamp()}"
        
        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(
            _process_batch_extraction,
            task_id,
            request.document_ids,
            request.user_id,
            request.extraction_types,
            extraction_service
        )
        
        result = {
            "task_id": task_id,
            "document_count": len(request.document_ids),
            "status": "processing",
            "message": f"æ‰¹é‡åˆ†æä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨å¤„ç† {len(request.document_ids)} ä¸ªæ–‡æ¡£",
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Batch analysis task created: {task_id}")
        return create_response(result)
        
    except Exception as e:
        logger.error(f"âŒ Batch analysis failed: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")


@router.get("/batch-status/{task_id}")
async def get_batch_status(task_id: str):
    """
    è·å–æ‰¹é‡ä»»åŠ¡çŠ¶æ€
    
    æŸ¥è¯¢æ‰¹é‡åˆ†æä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€å’Œè¿›åº¦ã€‚
    """
    try:
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–ç¼“å­˜ä¸­æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
        # æš‚æ—¶è¿”å›ç¤ºä¾‹çŠ¶æ€
        result = {
            "task_id": task_id,
            "status": "completed",  # processing, completed, failed
            "progress": 100,  # 0-100
            "processed_count": 5,
            "total_count": 5,
            "results": [
                {
                    "document_id": f"doc_{i}",
                    "status": "success",
                    "extraction_time": 2.5
                }
                for i in range(1, 6)
            ],
            "timestamp": datetime.now().isoformat()
        }
        
        return create_response(result)
        
    except Exception as e:
        logger.error(f"âŒ Batch status query failed: {e}")
        raise HTTPException(status_code=500, detail=f"ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢å¤±è´¥: {str(e)}")


@router.get("/statistics/{user_id}")
async def get_extraction_statistics(
    user_id: str,
    days: int = Query(30, description="ç»Ÿè®¡å¤©æ•°", ge=1, le=365)
):
    """
    è·å–æå–ç»Ÿè®¡ä¿¡æ¯
    
    è¿”å›ç”¨æˆ·çš„æ–‡æ¡£åˆ†æç»Ÿè®¡æ•°æ®ï¼ŒåŒ…æ‹¬å¤„ç†æ–‡æ¡£æ•°é‡ã€æå–ä¿¡æ¯ç»Ÿè®¡ç­‰ã€‚
    """
    try:
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æŸ¥è¯¢ç»Ÿè®¡æ•°æ®
        # æš‚æ—¶è¿”å›ç¤ºä¾‹æ•°æ®
        stats = {
            "user_id": user_id,
            "period_days": days,
            "total_documents": 25,
            "total_extractions": 45,
            "avg_processing_time": 3.2,
            "entity_types": {
                "ç»„ç»‡æœºæ„": 120,
                "è´¢åŠ¡æ•°æ®": 85,
                "æ”¿ç­–æ³•è§„": 65,
                "äººå‘˜èŒä½": 45,
                "æ—¶é—´æ—¥æœŸ": 95,
                "ESGæŒ‡æ ‡": 35
            },
            "information_categories": {
                "æ ¸å¿ƒä¸šåŠ¡": 30,
                "è´¢åŠ¡çŠ¶å†µ": 25,
                "é£é™©ç®¡ç†": 20,
                "æ²»ç†ç»“æ„": 15,
                "ESGè¡¨ç°": 18,
                "æˆ˜ç•¥è§„åˆ’": 12
            },
            "recent_activity": [
                {
                    "date": "2024-01-15",
                    "documents_processed": 3,
                    "extractions_count": 8
                },
                {
                    "date": "2024-01-14", 
                    "documents_processed": 2,
                    "extractions_count": 5
                }
            ],
            "timestamp": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… Retrieved extraction statistics for user: {user_id}")
        return create_response(stats)
        
    except Exception as e:
        logger.error(f"âŒ Statistics retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")


# ========== åå°ä»»åŠ¡å‡½æ•° ==========

async def _process_batch_extraction(
    task_id: str,
    document_ids: List[str],
    user_id: str,
    extraction_types: List[str],
    extraction_service: InformationExtractionService
):
    """å¤„ç†æ‰¹é‡æå–ä»»åŠ¡"""
    try:
        logger.info(f"ğŸ”„ Processing batch extraction task: {task_id}")
        
        results = []
        for i, doc_id in enumerate(document_ids):
            try:
                # æ‰§è¡Œå•ä¸ªæ–‡æ¡£æå–
                extraction_result = await extraction_service.extract_information(
                    document_id=doc_id,
                    user_id=user_id
                )
                
                results.append({
                    "document_id": doc_id,
                    "status": "success",
                    "extraction_time": extraction_result.processing_time
                })
                
                logger.info(f"âœ… Batch progress: {i+1}/{len(document_ids)} completed")
                
            except Exception as e:
                logger.error(f"âŒ Failed to process document {doc_id}: {e}")
                results.append({
                    "document_id": doc_id,
                    "status": "failed",
                    "error": str(e)
                })
        
        # è¿™é‡Œåº”è¯¥å°†ç»“æœä¿å­˜åˆ°æ•°æ®åº“æˆ–ç¼“å­˜
        logger.info(f"âœ… Batch extraction task completed: {task_id}")
        
    except Exception as e:
        logger.error(f"âŒ Batch extraction task failed: {task_id}, error: {e}") 