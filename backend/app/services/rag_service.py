"""
RAGæœåŠ¡ - æ£€ç´¢å¢å¼ºç”Ÿæˆæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
å®ç°åŸºäºæ–‡æ¡£å†…å®¹çš„ç²¾å‡†é—®ç­”ï¼Œæ”¯æŒæ–‡æ¡£åˆ†å—ç´¢å¼•ã€è¯­ä¹‰åŒ¹é…ã€ç­”æ¡ˆç”Ÿæˆå’Œå¼•ç”¨æº¯æº
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import asyncio

from app.vector_store.chroma_db import get_chroma_manager
from app.services.knowledge_service import get_knowledge_service
from app.services.document_processor import get_document_processor
from app.core.config import settings

logger = logging.getLogger(__name__)


class DocumentChunk:
    """æ–‡æ¡£åˆ†å—æ•°æ®ç»“æ„"""
    
    def __init__(self, content: str, metadata: Dict[str, Any]):
        self.content = content
        self.metadata = metadata
        self.chunk_id = metadata.get('chunk_id', '')
        self.document_id = metadata.get('document_id', '')
        self.page_number = metadata.get('page_number', 0)
        self.chunk_index = metadata.get('chunk_index', 0)
        self.similarity_score = 0.0
    
    def __str__(self):
        return f"Chunk({self.chunk_id}): {self.content[:100]}..."


class RAGAnswer:
    """RAGé—®ç­”ç»“æœæ•°æ®ç»“æ„"""
    
    def __init__(self, question: str, answer: str, sources: List[DocumentChunk], 
                 confidence: float, reasoning: str = ""):
        self.question = question
        self.answer = answer
        self.sources = sources
        self.confidence = confidence
        self.reasoning = reasoning
        self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'question': self.question,
            'answer': self.answer,
            'sources': [
                {
                    'content': chunk.content,
                    'document_id': chunk.document_id,
                    'page_number': chunk.page_number,
                    'chunk_index': chunk.chunk_index,
                    'similarity_score': chunk.similarity_score,
                    'metadata': chunk.metadata
                }
                for chunk in self.sources
            ],
            'confidence': self.confidence,
            'reasoning': self.reasoning,
            'timestamp': self.timestamp.isoformat()
        }


class RAGService:
    """
    RAGæœåŠ¡ - æ£€ç´¢å¢å¼ºç”Ÿæˆæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ–‡æ¡£åˆ†å—å’Œç´¢å¼•
    2. è¯­ä¹‰æ£€ç´¢å’ŒåŒ¹é…
    3. ä¸Šä¸‹æ–‡ç»„è£…å’Œç­”æ¡ˆç”Ÿæˆ
    4. å¼•ç”¨æº¯æºå’Œç½®ä¿¡åº¦è¯„ä¼°
    """
    
    def __init__(self):
        """åˆå§‹åŒ–RAGæœåŠ¡"""
        self.chroma_manager = None
        self.knowledge_service = None
        self.document_processor = None
        
        # RAGé…ç½®å‚æ•°
        self.chunk_size = 500  # æ–‡æ¡£åˆ†å—å¤§å°
        self.chunk_overlap = 50  # åˆ†å—é‡å å¤§å°
        self.max_context_length = 2000  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        self.min_similarity_threshold = 0.3  # æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
        self.max_retrieved_chunks = 5  # æœ€å¤§æ£€ç´¢åˆ†å—æ•°
        
        logger.info("ğŸ§  RAG Service initialized")
    
    async def _init_components(self):
        """å¼‚æ­¥åˆå§‹åŒ–ç»„ä»¶"""
        if not self.chroma_manager:
            self.chroma_manager = get_chroma_manager()
        if not self.knowledge_service:
            self.knowledge_service = get_knowledge_service()
        if not self.document_processor:
            self.document_processor = get_document_processor()
    
    async def answer_question(self, question: str, user_id: str, 
                            document_id: Optional[str] = None,
                            document_type: Optional[str] = None) -> RAGAnswer:
        """
        åŸºäºæ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            user_id: ç”¨æˆ·ID
            document_id: ç‰¹å®šæ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
            document_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            RAGAnswer: åŒ…å«ç­”æ¡ˆã€æ¥æºå’Œç½®ä¿¡åº¦çš„ç»“æœ
        """
        try:
            await self._init_components()
            
            logger.info(f"ğŸ¤” RAG Question: '{question}' (user: {user_id})")
            
            # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
            relevant_chunks = await self._retrieve_relevant_chunks(
                question, user_id, document_id, document_type
            )
            
            if not relevant_chunks:
                return self._generate_no_context_answer(question)
            
            # 2. ç»„è£…ä¸Šä¸‹æ–‡
            context = await self._assemble_context(relevant_chunks, question)
            
            # 3. ç”Ÿæˆç­”æ¡ˆ
            answer_text = await self._generate_answer(question, context, relevant_chunks)
            
            # 4. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(relevant_chunks, answer_text)
            
            # 5. ç”Ÿæˆæ¨ç†è§£é‡Š
            reasoning = self._generate_reasoning(question, relevant_chunks, confidence)
            
            rag_answer = RAGAnswer(
                question=question,
                answer=answer_text,
                sources=relevant_chunks,
                confidence=confidence,
                reasoning=reasoning
            )
            
            logger.info(f"âœ… RAG Answer generated (confidence: {confidence:.2%})")
            return rag_answer
            
        except Exception as e:
            logger.error(f"âŒ RAG question answering failed: {e}")
            return self._generate_error_answer(question, str(e))
    
    async def _retrieve_relevant_chunks(self, question: str, user_id: str,
                                      document_id: Optional[str] = None,
                                      document_type: Optional[str] = None) -> List[DocumentChunk]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            user_id: ç”¨æˆ·ID
            document_id: æ–‡æ¡£IDè¿‡æ»¤
            document_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤
            
        Returns:
            ç›¸å…³æ–‡æ¡£ç‰‡æ®µåˆ—è¡¨
        """
        try:
            # æ„å»ºæ£€ç´¢è¿‡æ»¤æ¡ä»¶
            where_filter = {
                "user_id": user_id,
                "status": "completed"
            }
            
            if document_id:
                where_filter["document_id"] = document_id
            
            if document_type:
                where_filter["file_type"] = document_type
            
            # æ‰§è¡Œå‘é‡æ£€ç´¢
            search_results = self.chroma_manager.collection.query(
                query_texts=[question],
                n_results=self.max_retrieved_chunks,
                where=where_filter
            )
            
            # è½¬æ¢ä¸ºDocumentChunkå¯¹è±¡
            chunks = []
            if search_results.get('documents') and search_results['documents'][0]:
                documents = search_results['documents'][0]
                metadatas = search_results.get('metadatas', [[]])[0]
                distances = search_results.get('distances', [[]])[0]
                
                for i, (doc, metadata, distance) in enumerate(zip(documents, metadatas, distances)):
                    # è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
                    similarity_score = max(0, 1 - distance) if distance is not None else 0
                    
                    # è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„ç»“æœ
                    if similarity_score < self.min_similarity_threshold:
                        continue
                    
                    chunk = DocumentChunk(content=doc, metadata=metadata)
                    chunk.similarity_score = similarity_score
                    chunks.append(chunk)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            chunks.sort(key=lambda x: x.similarity_score, reverse=True)
            
            logger.info(f"ğŸ” Retrieved {len(chunks)} relevant chunks")
            return chunks
            
        except Exception as e:
            logger.error(f"âŒ Chunk retrieval failed: {e}")
            return []
    
    async def _assemble_context(self, chunks: List[DocumentChunk], question: str) -> str:
        """
        ç»„è£…é—®ç­”ä¸Šä¸‹æ–‡
        
        Args:
            chunks: ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç»„è£…åçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        try:
            context_parts = []
            current_length = 0
            
            # æ·»åŠ é—®é¢˜ä½œä¸ºä¸Šä¸‹æ–‡çš„å¼€å¤´
            context_parts.append(f"é—®é¢˜: {question}\n")
            current_length += len(question) + 10
            
            context_parts.append("ç›¸å…³æ–‡æ¡£å†…å®¹:\n")
            current_length += 20
            
            # æŒ‰ç›¸ä¼¼åº¦é¡ºåºæ·»åŠ æ–‡æ¡£ç‰‡æ®µ
            for i, chunk in enumerate(chunks):
                # æ„å»ºç‰‡æ®µä¿¡æ¯
                source_info = f"[æ¥æº {i+1}] "
                if chunk.metadata.get('filename'):
                    source_info += f"æ–‡æ¡£: {chunk.metadata['filename']}"
                if chunk.page_number:
                    source_info += f", ç¬¬{chunk.page_number}é¡µ"
                source_info += f" (ç›¸å…³åº¦: {chunk.similarity_score:.1%})\n"
                
                chunk_content = f"{source_info}å†…å®¹: {chunk.content}\n\n"
                
                # æ£€æŸ¥é•¿åº¦é™åˆ¶
                if current_length + len(chunk_content) > self.max_context_length:
                    break
                
                context_parts.append(chunk_content)
                current_length += len(chunk_content)
            
            context = "".join(context_parts)
            logger.info(f"ğŸ“ Context assembled: {len(context)} characters")
            return context
            
        except Exception as e:
            logger.error(f"âŒ Context assembly failed: {e}")
            return f"é—®é¢˜: {question}\nç›¸å…³æ–‡æ¡£å†…å®¹: [ä¸Šä¸‹æ–‡ç»„è£…å¤±è´¥]"
    
    async def _generate_answer(self, question: str, context: str, 
                             sources: List[DocumentChunk]) -> str:
        """
        åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context: ç»„è£…çš„ä¸Šä¸‹æ–‡
            sources: æ¥æºæ–‡æ¡£ç‰‡æ®µ
            
        Returns:
            ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨LLM APIæ¥ç”Ÿæˆç­”æ¡ˆ
            # æš‚æ—¶ä½¿ç”¨åŸºäºè§„åˆ™çš„ç®€å•ç­”æ¡ˆç”Ÿæˆ
            
            # åˆ†æé—®é¢˜ç±»å‹
            question_type = self._analyze_question_type(question)
            
            # æå–å…³é”®ä¿¡æ¯
            key_info = self._extract_key_information(sources, question)
            
            # æ„å»ºç­”æ¡ˆ
            if question_type == "definition":
                answer = self._generate_definition_answer(question, key_info, sources)
            elif question_type == "process":
                answer = self._generate_process_answer(question, key_info, sources)
            elif question_type == "comparison":
                answer = self._generate_comparison_answer(question, key_info, sources)
            elif question_type == "factual":
                answer = self._generate_factual_answer(question, key_info, sources)
            else:
                answer = self._generate_general_answer(question, key_info, sources)
            
            # æ·»åŠ æ¥æºå¼•ç”¨
            answer_with_sources = self._add_source_citations(answer, sources)
            
            return answer_with_sources
            
        except Exception as e:
            logger.error(f"âŒ Answer generation failed: {e}")
            return f"åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œæˆ‘æ— æ³•å‡†ç¡®å›ç­”æ‚¨çš„é—®é¢˜ï¼š{question}ã€‚è¯·å°è¯•é‡æ–°è¡¨è¿°é—®é¢˜æˆ–æ£€æŸ¥ç›¸å…³æ–‡æ¡£æ˜¯å¦åŒ…å«æ‰€éœ€ä¿¡æ¯ã€‚"
    
    def _analyze_question_type(self, question: str) -> str:
        """åˆ†æé—®é¢˜ç±»å‹"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["ä»€ä¹ˆæ˜¯", "å®šä¹‰", "å«ä¹‰", "æ¦‚å¿µ"]):
            return "definition"
        elif any(word in question_lower for word in ["å¦‚ä½•", "æ€æ ·", "æµç¨‹", "æ­¥éª¤", "è¿‡ç¨‹"]):
            return "process"
        elif any(word in question_lower for word in ["åŒºåˆ«", "å·®å¼‚", "æ¯”è¾ƒ", "å¯¹æ¯”"]):
            return "comparison"
        elif any(word in question_lower for word in ["å¤šå°‘", "ä½•æ—¶", "å“ªé‡Œ", "è°"]):
            return "factual"
        else:
            return "general"
    
    def _extract_key_information(self, sources: List[DocumentChunk], question: str) -> Dict[str, Any]:
        """ä»æ¥æºæ–‡æ¡£ä¸­æå–å…³é”®ä¿¡æ¯"""
        key_info = {
            "main_concepts": [],
            "numbers": [],
            "dates": [],
            "entities": [],
            "processes": []
        }
        
        for chunk in sources:
            content = chunk.content
            
            # æå–æ•°å­—ä¿¡æ¯
            numbers = re.findall(r'\d+(?:\.\d+)?%?', content)
            key_info["numbers"].extend(numbers)
            
            # æå–æ—¥æœŸä¿¡æ¯
            dates = re.findall(r'\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥', content)
            key_info["dates"].extend(dates)
            
            # ç®€å•çš„å®ä½“è¯†åˆ«ï¼ˆå¯ä»¥åç»­é›†æˆNERæ¨¡å‹ï¼‰
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„è§„åˆ™è¯†åˆ«
            if "å…¬å¸" in content or "ä¼ä¸š" in content:
                key_info["entities"].append("ä¼ä¸šå®ä½“")
            if "æ”¿ç­–" in content or "åˆ¶åº¦" in content:
                key_info["entities"].append("æ”¿ç­–åˆ¶åº¦")
        
        return key_info
    
    def _generate_definition_answer(self, question: str, key_info: Dict[str, Any], 
                                  sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆå®šä¹‰ç±»é—®é¢˜çš„ç­”æ¡ˆ"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å®šä¹‰ä¿¡æ¯ã€‚"
        
        # å–æœ€ç›¸å…³çš„ç‰‡æ®µä½œä¸ºå®šä¹‰æ¥æº
        main_source = sources[0]
        
        answer_parts = [
            f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œ{question.replace('ä»€ä¹ˆæ˜¯', '').replace('ï¼Ÿ', '').strip()}çš„å®šä¹‰å¦‚ä¸‹ï¼š",
            "",
            main_source.content.strip(),
            ""
        ]
        
        # å¦‚æœæœ‰å¤šä¸ªæ¥æºï¼Œæ·»åŠ è¡¥å……ä¿¡æ¯
        if len(sources) > 1:
            answer_parts.append("è¡¥å……ä¿¡æ¯ï¼š")
            for i, source in enumerate(sources[1:3], 2):  # æœ€å¤šæ·»åŠ 2ä¸ªè¡¥å……æ¥æº
                answer_parts.append(f"â€¢ {source.content.strip()}")
        
        return "\n".join(answer_parts)
    
    def _generate_process_answer(self, question: str, key_info: Dict[str, Any], 
                               sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆæµç¨‹ç±»é—®é¢˜çš„ç­”æ¡ˆ"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æµç¨‹ä¿¡æ¯ã€‚"
        
        answer_parts = [
            f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œå…³äº{question.replace('å¦‚ä½•', '').replace('æ€æ ·', '').replace('ï¼Ÿ', '').strip()}çš„æµç¨‹å¦‚ä¸‹ï¼š",
            ""
        ]
        
        # å°è¯•è¯†åˆ«æ­¥éª¤
        for i, source in enumerate(sources[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªæ¥æº
            content = source.content.strip()
            
            # å¦‚æœå†…å®¹åŒ…å«æ˜æ˜¾çš„æ­¥éª¤æ ‡è¯†
            if any(step_word in content for step_word in ["ç¬¬ä¸€", "ç¬¬äºŒ", "é¦–å…ˆ", "ç„¶å", "æœ€å", "æ­¥éª¤"]):
                answer_parts.append(f"**æ­¥éª¤ {i+1}**: {content}")
            else:
                answer_parts.append(f"**è¦ç‚¹ {i+1}**: {content}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)
    
    def _generate_comparison_answer(self, question: str, key_info: Dict[str, Any], 
                                  sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆæ¯”è¾ƒç±»é—®é¢˜çš„ç­”æ¡ˆ"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ¯”è¾ƒä¿¡æ¯ã€‚"
        
        answer_parts = [
            f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œå…³äº{question.replace('ï¼Ÿ', '').strip()}çš„æ¯”è¾ƒåˆ†æå¦‚ä¸‹ï¼š",
            ""
        ]
        
        for i, source in enumerate(sources[:2]):  # æœ€å¤šä½¿ç”¨2ä¸ªæ¥æºè¿›è¡Œæ¯”è¾ƒ
            answer_parts.append(f"**æ–¹é¢ {i+1}**: {source.content.strip()}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)
    
    def _generate_factual_answer(self, question: str, key_info: Dict[str, Any], 
                               sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆäº‹å®ç±»é—®é¢˜çš„ç­”æ¡ˆ"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„äº‹å®ä¿¡æ¯ã€‚"
        
        # ä¼˜å…ˆä½¿ç”¨åŒ…å«æ•°å­—ã€æ—¥æœŸç­‰å…·ä½“ä¿¡æ¯çš„ç‰‡æ®µ
        factual_sources = []
        for source in sources:
            if any(num in source.content for num in key_info.get("numbers", [])) or \
               any(date in source.content for date in key_info.get("dates", [])):
                factual_sources.append(source)
        
        if not factual_sources:
            factual_sources = sources[:2]
        
        answer_parts = [
            f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œ{question.replace('ï¼Ÿ', '').strip()}çš„ç›¸å…³ä¿¡æ¯å¦‚ä¸‹ï¼š",
            ""
        ]
        
        for source in factual_sources:
            answer_parts.append(f"â€¢ {source.content.strip()}")
        
        return "\n".join(answer_parts)
    
    def _generate_general_answer(self, question: str, key_info: Dict[str, Any], 
                               sources: List[DocumentChunk]) -> str:
        """ç”Ÿæˆä¸€èˆ¬é—®é¢˜çš„ç­”æ¡ˆ"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
        
        answer_parts = [
            f"æ ¹æ®æ–‡æ¡£å†…å®¹ï¼Œå…³äºæ‚¨çš„é—®é¢˜ã€Œ{question}ã€ï¼Œæˆ‘æ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š",
            ""
        ]
        
        for i, source in enumerate(sources[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªæ¥æº
            answer_parts.append(f"**ä¿¡æ¯ {i+1}**: {source.content.strip()}")
            answer_parts.append("")
        
        return "\n".join(answer_parts)
    
    def _add_source_citations(self, answer: str, sources: List[DocumentChunk]) -> str:
        """ä¸ºç­”æ¡ˆæ·»åŠ æ¥æºå¼•ç”¨"""
        if not sources:
            return answer
        
        citation_parts = [
            answer,
            "",
            "ğŸ“š **å‚è€ƒæ¥æº**:"
        ]
        
        for i, source in enumerate(sources):
            filename = source.metadata.get('filename', 'æœªçŸ¥æ–‡æ¡£')
            page_info = f"ç¬¬{source.page_number}é¡µ" if source.page_number else ""
            similarity = f"ç›¸å…³åº¦: {source.similarity_score:.1%}"
            
            citation = f"[{i+1}] {filename}"
            if page_info:
                citation += f" - {page_info}"
            citation += f" ({similarity})"
            
            citation_parts.append(citation)
        
        return "\n".join(citation_parts)
    
    def _calculate_confidence(self, sources: List[DocumentChunk], answer: str) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦"""
        if not sources:
            return 0.0
        
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—ç½®ä¿¡åº¦
        factors = []
        
        # 1. å¹³å‡ç›¸ä¼¼åº¦å¾—åˆ†
        avg_similarity = sum(chunk.similarity_score for chunk in sources) / len(sources)
        factors.append(avg_similarity)
        
        # 2. æ¥æºæ•°é‡å› å­ï¼ˆæ›´å¤šæ¥æºé€šå¸¸æ„å‘³ç€æ›´é«˜ç½®ä¿¡åº¦ï¼Œä½†æœ‰ä¸Šé™ï¼‰
        source_count_factor = min(len(sources) / 3.0, 1.0)
        factors.append(source_count_factor)
        
        # 3. æœ€é«˜ç›¸ä¼¼åº¦å¾—åˆ†çš„æƒé‡
        max_similarity = max(chunk.similarity_score for chunk in sources)
        factors.append(max_similarity * 0.8)
        
        # 4. ç­”æ¡ˆé•¿åº¦å› å­ï¼ˆè¿‡çŸ­æˆ–è¿‡é•¿çš„ç­”æ¡ˆå¯èƒ½ç½®ä¿¡åº¦è¾ƒä½ï¼‰
        answer_length = len(answer)
        if 100 <= answer_length <= 1000:
            length_factor = 1.0
        elif answer_length < 100:
            length_factor = answer_length / 100.0
        else:
            length_factor = max(0.5, 1000.0 / answer_length)
        factors.append(length_factor)
        
        # è®¡ç®—åŠ æƒå¹³å‡ç½®ä¿¡åº¦
        weights = [0.4, 0.2, 0.3, 0.1]  # å„å› å­çš„æƒé‡
        confidence = sum(f * w for f, w in zip(factors, weights))
        
        return min(max(confidence, 0.0), 1.0)  # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
    
    def _generate_reasoning(self, question: str, sources: List[DocumentChunk], 
                          confidence: float) -> str:
        """ç”Ÿæˆæ¨ç†è§£é‡Š"""
        reasoning_parts = []
        
        if not sources:
            return "æ— æ³•æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹æ¥å›ç­”é—®é¢˜ã€‚"
        
        # æ£€ç´¢æƒ…å†µè¯´æ˜
        reasoning_parts.append(f"æ£€ç´¢åˆ°{len(sources)}ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        
        # ç›¸ä¼¼åº¦åˆ†æ
        similarities = [chunk.similarity_score for chunk in sources]
        avg_sim = sum(similarities) / len(similarities)
        max_sim = max(similarities)
        
        reasoning_parts.append(f"å¹³å‡ç›¸å…³åº¦: {avg_sim:.1%}, æœ€é«˜ç›¸å…³åº¦: {max_sim:.1%}")
        
        # ç½®ä¿¡åº¦è§£é‡Š
        if confidence >= 0.8:
            reasoning_parts.append("é«˜ç½®ä¿¡åº¦ï¼šæ‰¾åˆ°å¤šä¸ªé«˜åº¦ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ")
        elif confidence >= 0.6:
            reasoning_parts.append("ä¸­ç­‰ç½®ä¿¡åº¦ï¼šæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œä½†ç›¸å…³æ€§æœ‰é™")
        elif confidence >= 0.4:
            reasoning_parts.append("ä½ç½®ä¿¡åº¦ï¼šæ–‡æ¡£ç‰‡æ®µç›¸å…³æ€§è¾ƒå¼±")
        else:
            reasoning_parts.append("æä½ç½®ä¿¡åº¦ï¼šæœªæ‰¾åˆ°é«˜åº¦ç›¸å…³çš„æ–‡æ¡£å†…å®¹")
        
        return "ï¼›".join(reasoning_parts)
    
    def _generate_no_context_answer(self, question: str) -> RAGAnswer:
        """ç”Ÿæˆæ— ä¸Šä¸‹æ–‡æ—¶çš„ç­”æ¡ˆ"""
        return RAGAnswer(
            question=question,
            answer="æŠ±æ­‰ï¼Œæˆ‘åœ¨æ‚¨çš„çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ­¤é—®é¢˜ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚è¯·ç¡®è®¤ï¼š\n"
                  "1. ç›¸å…³æ–‡æ¡£å·²ç»ä¸Šä¼ å¹¶å®Œæˆå¤„ç†\n"
                  "2. é—®é¢˜è¡¨è¿°æ¸…æ™°ä¸”ä¸æ–‡æ¡£å†…å®¹ç›¸å…³\n"
                  "3. å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æé—®",
            sources=[],
            confidence=0.0,
            reasoning="æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£å†…å®¹"
        )
    
    def _generate_error_answer(self, question: str, error_message: str) -> RAGAnswer:
        """ç”Ÿæˆé”™è¯¯æ—¶çš„ç­”æ¡ˆ"""
        return RAGAnswer(
            question=question,
            answer=f"å¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°é”™è¯¯ï¼š{error_message}ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
            sources=[],
            confidence=0.0,
            reasoning=f"ç³»ç»Ÿé”™è¯¯: {error_message}"
        )


# å…¨å±€RAGæœåŠ¡å®ä¾‹
_rag_service_instance = None


def get_rag_service() -> RAGService:
    """è·å–RAGæœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _rag_service_instance
    if _rag_service_instance is None:
        _rag_service_instance = RAGService()
    return _rag_service_instance 