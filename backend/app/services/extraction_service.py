"""
ä¿¡æ¯æå–ä¸æ‘˜è¦ç”ŸæˆæœåŠ¡
æ”¯æŒæ–‡æ¡£å…³é”®ä¿¡æ¯æå–ã€å®ä½“è¯†åˆ«ã€æ ‡ç­¾ç”Ÿæˆå’Œå¤šå±‚æ¬¡æ‘˜è¦ç”Ÿæˆ
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, asdict
import asyncio

from app.services.knowledge_service import get_knowledge_service
from app.vector_store.chroma_db import get_chroma_manager
from app.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class ExtractedEntity:
    """æå–çš„å®ä½“ä¿¡æ¯"""
    text: str  # å®ä½“æ–‡æœ¬
    type: str  # å®ä½“ç±»å‹
    confidence: float  # ç½®ä¿¡åº¦
    context: str  # ä¸Šä¸‹æ–‡
    position: int  # åœ¨æ–‡æ¡£ä¸­çš„ä½ç½®


@dataclass 
class KeyInformation:
    """å…³é”®ä¿¡æ¯ç‚¹"""
    content: str  # ä¿¡æ¯å†…å®¹
    importance: float  # é‡è¦æ€§è¯„åˆ†
    category: str  # ä¿¡æ¯ç±»åˆ«
    keywords: List[str]  # å…³é”®è¯
    source_section: str  # æ¥æºç« èŠ‚


@dataclass
class DocumentSummary:
    """æ–‡æ¡£æ‘˜è¦"""
    title: str  # æ–‡æ¡£æ ‡é¢˜
    brief_summary: str  # ç®€è¦æ‘˜è¦ï¼ˆ1-2å¥è¯ï¼‰
    detailed_summary: str  # è¯¦ç»†æ‘˜è¦ï¼ˆæ®µè½çº§åˆ«ï¼‰
    key_points: List[str]  # å…³é”®è¦ç‚¹
    structure_summary: str  # ç»“æ„æ€§æ‘˜è¦
    confidence: float  # æ‘˜è¦è´¨é‡ç½®ä¿¡åº¦


@dataclass
class ExtractionResult:
    """ä¿¡æ¯æå–ç»“æœ"""
    document_id: str
    document_name: str
    
    # åŸºç¡€ä¿¡æ¯
    summary: DocumentSummary
    key_information: List[KeyInformation]
    entities: List[ExtractedEntity]
    tags: List[str]
    
    # ç»Ÿè®¡ä¿¡æ¯
    word_count: int
    paragraph_count: int
    section_count: int
    
    # å…ƒæ•°æ®
    extraction_timestamp: datetime
    processing_time: float
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)


class InformationExtractionService:
    """
    ä¿¡æ¯æå–ä¸æ‘˜è¦ç”ŸæˆæœåŠ¡
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å…³é”®ä¿¡æ¯æå–
    2. å®ä½“è¯†åˆ«ä¸åˆ†ç±»
    3. è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆ
    4. å¤šå±‚æ¬¡æ‘˜è¦ç”Ÿæˆ
    5. ç»“æ„åŒ–ä¿¡æ¯è¾“å‡º
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ä¿¡æ¯æå–æœåŠ¡"""
        self.knowledge_service = None
        self.chroma_manager = None
        
        # é¢„å®šä¹‰çš„å®ä½“ç±»å‹å’Œæ¨¡å¼
        self.entity_patterns = {
            "ç»„ç»‡æœºæ„": [
                r"å…¬å¸|ä¼ä¸š|é›†å›¢|æœ‰é™å…¬å¸|è‚¡ä»½å…¬å¸|æœºæ„|éƒ¨é—¨|å§”å‘˜ä¼š",
                r"[A-Z][a-zA-Z\s]+(?:å…¬å¸|Company|Corp|Inc|Ltd)"
            ],
            "äººå‘˜èŒä½": [
                r"æ€»ç»ç†|è‘£äº‹é•¿|CEO|CTO|CFO|æ€»ç›‘|ç»ç†|ä¸»ç®¡|è´Ÿè´£äºº|ä¸“å‘˜",
                r"è‘£äº‹|ç›‘äº‹|è‚¡ä¸œ|æŠ•èµ„è€…|å‘˜å·¥|ç®¡ç†å±‚"
            ],
            "æ”¿ç­–æ³•è§„": [
                r"æ³•å¾‹|æ³•è§„|æ¡ä¾‹|è§„å®š|åˆ¶åº¦|æ”¿ç­–|æ ‡å‡†|è§„èŒƒ|å‡†åˆ™|æŒ‡å¯¼æ„è§",
                r"ã€Š[^ã€‹]+ã€‹|ç¬¬\d+æ¡|ç¬¬\d+æ¬¾|ç¬¬\d+é¡¹"
            ],
            "è´¢åŠ¡æ•°æ®": [
                r"\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒä¸‡)?å…ƒ",
                r"\d+(?:\.\d+)?%",
                r"è¥æ”¶|åˆ©æ¶¦|æˆæœ¬|è´¹ç”¨|èµ„äº§|è´Ÿå€º|ç°é‡‘æµ"
            ],
            "æ—¶é—´æ—¥æœŸ": [
                r"\d{4}å¹´(?:\d{1,2}æœˆ)?(?:\d{1,2}æ—¥)?",
                r"\d{1,2}æœˆ\d{1,2}æ—¥",
                r"ç¬¬[ä¸€äºŒä¸‰å››]å­£åº¦|ä¸ŠåŠå¹´|ä¸‹åŠå¹´|å¹´åº¦"
            ],
            "ESGæŒ‡æ ‡": [
                r"ç¢³æ’æ”¾|èŠ‚èƒ½å‡æ’|ç¯å¢ƒä¿æŠ¤|å¯æŒç»­å‘å±•|ç¤¾ä¼šè´£ä»»",
                r"å…¬å¸æ²»ç†|åˆè§„|é£é™©ç®¡ç†|å†…æ§|å®¡è®¡"
            ]
        }
        
        # å…³é”®ä¿¡æ¯ç±»åˆ«
        self.information_categories = {
            "æ ¸å¿ƒä¸šåŠ¡": ["ä¸šåŠ¡", "äº§å“", "æœåŠ¡", "å¸‚åœº", "å®¢æˆ·"],
            "è´¢åŠ¡çŠ¶å†µ": ["è´¢åŠ¡", "æ”¶å…¥", "åˆ©æ¶¦", "æˆæœ¬", "æŠ•èµ„"],
            "é£é™©ç®¡ç†": ["é£é™©", "åˆè§„", "å†…æ§", "å®¡è®¡", "ç›‘ç®¡"],
            "æ²»ç†ç»“æ„": ["æ²»ç†", "è‘£äº‹ä¼š", "ç®¡ç†å±‚", "è‚¡ä¸œ", "å†³ç­–"],
            "ESGè¡¨ç°": ["ç¯å¢ƒ", "ç¤¾ä¼š", "æ²»ç†", "å¯æŒç»­", "è´£ä»»"],
            "æˆ˜ç•¥è§„åˆ’": ["æˆ˜ç•¥", "è§„åˆ’", "ç›®æ ‡", "å‘å±•", "åˆ›æ–°"]
        }
        
        logger.info("ğŸ§  Information Extraction Service initialized")
    
    async def _init_components(self):
        """å¼‚æ­¥åˆå§‹åŒ–ç»„ä»¶"""
        if not self.knowledge_service:
            self.knowledge_service = get_knowledge_service()
        if not self.chroma_manager:
            self.chroma_manager = get_chroma_manager()
    
    async def extract_information(self, document_id: str, user_id: str) -> ExtractionResult:
        """
        æå–æ–‡æ¡£çš„å…³é”®ä¿¡æ¯
        
        Args:
            document_id: æ–‡æ¡£ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            ExtractionResult: æå–ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            await self._init_components()
            
            logger.info(f"ğŸ” Starting information extraction for document: {document_id}")
            
            # 1. è·å–æ–‡æ¡£å†…å®¹
            document_content = await self._get_document_content(document_id, user_id)
            if not document_content:
                raise ValueError(f"Document {document_id} not found or empty")
            
            # 2. å¹¶è¡Œæ‰§è¡Œå„ç§æå–ä»»åŠ¡
            extraction_tasks = [
                self._extract_summary(document_content),
                self._extract_key_information(document_content),
                self._extract_entities(document_content),
                self._generate_tags(document_content),
                self._analyze_document_structure(document_content)
            ]
            
            results = await asyncio.gather(*extraction_tasks)
            summary, key_info, entities, tags, structure_stats = results
            
            # 3. æ„å»ºæå–ç»“æœ
            processing_time = (datetime.now() - start_time).total_seconds()
            
            extraction_result = ExtractionResult(
                document_id=document_id,
                document_name=document_content.get('name', 'Unknown Document'),
                summary=summary,
                key_information=key_info,
                entities=entities,
                tags=tags,
                word_count=structure_stats['word_count'],
                paragraph_count=structure_stats['paragraph_count'],
                section_count=structure_stats['section_count'],
                extraction_timestamp=datetime.now(),
                processing_time=processing_time
            )
            
            logger.info(f"âœ… Information extraction completed in {processing_time:.2f}s")
            return extraction_result
            
        except Exception as e:
            logger.error(f"âŒ Information extraction failed: {e}")
            raise
    
    async def _get_document_content(self, document_id: str, user_id: str) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£å†…å®¹"""
        try:
            # ä»å‘é‡æ•°æ®åº“è·å–æ–‡æ¡£ç‰‡æ®µ
            search_results = self.chroma_manager.collection.query(
                query_texts=[""],  # ç©ºæŸ¥è¯¢è·å–æ‰€æœ‰ç‰‡æ®µ
                n_results=100,  # è·å–æ›´å¤šç‰‡æ®µ
                where={
                    "document_id": document_id,
                    "user_id": user_id
                }
            )
            
            if not search_results.get('documents') or not search_results['documents'][0]:
                return {}
            
            # åˆå¹¶æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µ
            documents = search_results['documents'][0]
            metadatas = search_results.get('metadatas', [[]])[0]
            
            # æŒ‰chunk_indexæ’åºï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            combined_docs = list(zip(documents, metadatas))
            combined_docs.sort(key=lambda x: x[1].get('chunk_index', 0))
            
            full_content = "\n\n".join([doc for doc, _ in combined_docs])
            
            # è·å–æ–‡æ¡£å…ƒä¿¡æ¯
            document_name = metadatas[0].get('filename', 'Unknown Document') if metadatas else 'Unknown Document'
            
            return {
                'content': full_content,
                'name': document_name,
                'metadata': metadatas[0] if metadatas else {}
            }
            
        except Exception as e:
            logger.error(f"âŒ Failed to get document content: {e}")
            return {}
    
    async def _extract_summary(self, document_content: Dict[str, Any]) -> DocumentSummary:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        try:
            content = document_content.get('content', '')
            doc_name = document_content.get('name', 'Unknown Document')
            
            if not content:
                return DocumentSummary(
                    title=doc_name,
                    brief_summary="æ–‡æ¡£å†…å®¹ä¸ºç©º",
                    detailed_summary="æ— æ³•ç”Ÿæˆæ‘˜è¦ï¼Œæ–‡æ¡£å†…å®¹ä¸ºç©ºã€‚",
                    key_points=[],
                    structure_summary="æ–‡æ¡£ç»“æ„ï¼šæ— å†…å®¹",
                    confidence=0.0
                )
            
            # åˆ†ææ–‡æ¡£ç»“æ„
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            sentences = [s.strip() for p in paragraphs for s in p.split('ã€‚') if s.strip()]
            
            # ç®€è¦æ‘˜è¦ - å–å‰å‡ ä¸ªé‡è¦å¥å­
            brief_sentences = sentences[:2] if len(sentences) >= 2 else sentences
            brief_summary = 'ã€‚'.join(brief_sentences)
            if brief_summary and not brief_summary.endswith('ã€‚'):
                brief_summary += 'ã€‚'
            
            # è¯¦ç»†æ‘˜è¦ - åŸºäºæ®µè½ç”Ÿæˆ
            important_paragraphs = self._select_important_paragraphs(paragraphs)
            detailed_summary = self._generate_detailed_summary(important_paragraphs)
            
            # å…³é”®è¦ç‚¹æå–
            key_points = self._extract_key_points(content)
            
            # ç»“æ„æ€§æ‘˜è¦
            structure_summary = self._generate_structure_summary(content)
            
            # è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_summary_confidence(content, brief_summary, detailed_summary)
            
            return DocumentSummary(
                title=doc_name,
                brief_summary=brief_summary or "æ–‡æ¡£æ¦‚è¿°ä¿¡æ¯ä¸è¶³",
                detailed_summary=detailed_summary,
                key_points=key_points,
                structure_summary=structure_summary,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"âŒ Summary extraction failed: {e}")
            return DocumentSummary(
                title=document_content.get('name', 'Unknown Document'),
                brief_summary="æ‘˜è¦ç”Ÿæˆå¤±è´¥",
                detailed_summary="ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£æ‘˜è¦ã€‚",
                key_points=[],
                structure_summary="ç»“æ„åˆ†æå¤±è´¥",
                confidence=0.0
            )
    
    def _select_important_paragraphs(self, paragraphs: List[str], max_paragraphs: int = 5) -> List[str]:
        """é€‰æ‹©é‡è¦æ®µè½"""
        if not paragraphs:
            return []
        
        # æ®µè½é‡è¦æ€§è¯„åˆ†
        paragraph_scores = []
        
        for paragraph in paragraphs:
            score = 0
            
            # é•¿åº¦è¯„åˆ†ï¼ˆé€‚ä¸­é•¿åº¦æ›´é‡è¦ï¼‰
            length = len(paragraph)
            if 50 <= length <= 500:
                score += 2
            elif 20 <= length <= 50 or 500 <= length <= 1000:
                score += 1
            
            # å…³é”®è¯è¯„åˆ†
            important_keywords = [
                "é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "åŸºæœ¬", "æ ¹æœ¬", "é¦–è¦",
                "ç›®æ ‡", "åŸåˆ™", "è¦æ±‚", "è§„å®š", "æ ‡å‡†", "æµç¨‹", "åˆ¶åº¦",
                "æ€»ç»“", "ç»“è®º", "å»ºè®®", "æªæ–½", "æ–¹æ¡ˆ", "è®¡åˆ’"
            ]
            
            for keyword in important_keywords:
                if keyword in paragraph:
                    score += 1
            
            # æ•°å­—å’Œç™¾åˆ†æ¯”ï¼ˆé€šå¸¸åŒ…å«é‡è¦ä¿¡æ¯ï¼‰
            if re.search(r'\d+(?:\.\d+)?%', paragraph):
                score += 1
            if re.search(r'\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒä¸‡)?å…ƒ', paragraph):
                score += 1
            
            # åˆ—è¡¨æˆ–æ¡ç›®ï¼ˆé€šå¸¸æ˜¯è¦ç‚¹ï¼‰
            if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|[1-9]\.|[1-9]ï¼‰', paragraph):
                score += 1
            
            paragraph_scores.append((paragraph, score))
        
        # æŒ‰è¯„åˆ†æ’åºå¹¶é€‰æ‹©å‰Nä¸ª
        paragraph_scores.sort(key=lambda x: x[1], reverse=True)
        selected = [p for p, _ in paragraph_scores[:max_paragraphs]]
        
        return selected
    
    def _generate_detailed_summary(self, important_paragraphs: List[str]) -> str:
        """ç”Ÿæˆè¯¦ç»†æ‘˜è¦"""
        if not important_paragraphs:
            return "æ–‡æ¡£å†…å®¹åˆ†æä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆè¯¦ç»†æ‘˜è¦ã€‚"
        
        # ç®€åŒ–ç‰ˆæ‘˜è¦ç”Ÿæˆ - æå–æ¯ä¸ªæ®µè½çš„æ ¸å¿ƒå¥å­
        summary_parts = []
        
        for paragraph in important_paragraphs:
            # å–æ®µè½çš„ç¬¬ä¸€å¥æˆ–æœ€é‡è¦çš„å¥å­
            sentences = [s.strip() for s in paragraph.split('ã€‚') if s.strip()]
            if sentences:
                # é€‰æ‹©æœ€é•¿çš„å¥å­ä½œä¸ºä»£è¡¨ï¼ˆé€šå¸¸åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
                representative_sentence = max(sentences, key=len)
                if len(representative_sentence) > 10:  # è¿‡æ»¤è¿‡çŸ­çš„å¥å­
                    summary_parts.append(representative_sentence)
        
        # ç»„åˆæ‘˜è¦
        if summary_parts:
            detailed_summary = 'ã€‚'.join(summary_parts[:3])  # æœ€å¤š3å¥
            if not detailed_summary.endswith('ã€‚'):
                detailed_summary += 'ã€‚'
            return detailed_summary
        else:
            return "åŸºäºæ–‡æ¡£å†…å®¹åˆ†æï¼Œæœªèƒ½æå–åˆ°è¶³å¤Ÿçš„å…³é”®ä¿¡æ¯ç”Ÿæˆè¯¦ç»†æ‘˜è¦ã€‚"
    
    def _extract_key_points(self, content: str) -> List[str]:
        """æå–å…³é”®è¦ç‚¹"""
        key_points = []
        
        # æŸ¥æ‰¾æ˜æ˜¾çš„è¦ç‚¹æ ‡è®°
        patterns = [
            r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€([^ã€‚]+)',
            r'[1-9]\.\s*([^ã€‚]+)',
            r'[1-9]ï¼‰\s*([^ã€‚]+)',
            r'â€¢\s*([^ã€‚]+)',
            r'â—†\s*([^ã€‚]+)',
            r'â˜…\s*([^ã€‚]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                point = match.strip()
                if len(point) > 5 and len(point) < 200:  # è¿‡æ»¤è¿‡çŸ­æˆ–è¿‡é•¿çš„å†…å®¹
                    key_points.append(point)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„è¦ç‚¹ï¼Œä»é‡è¦å¥å­ä¸­æå–
        if not key_points:
            sentences = [s.strip() for s in content.split('ã€‚') if s.strip()]
            
            # æŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å¥å­
            important_keywords = ["é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "å¿…é¡»", "åº”å½“", "éœ€è¦"]
            
            for sentence in sentences:
                if len(sentence) > 10 and len(sentence) < 150:
                    for keyword in important_keywords:
                        if keyword in sentence:
                            key_points.append(sentence)
                            break
                
                if len(key_points) >= 5:  # é™åˆ¶æ•°é‡
                    break
        
        return key_points[:5]  # è¿”å›å‰5ä¸ªè¦ç‚¹
    
    def _generate_structure_summary(self, content: str) -> str:
        """ç”Ÿæˆç»“æ„æ€§æ‘˜è¦"""
        try:
            # åˆ†ææ–‡æ¡£ç»“æ„
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            sentences = [s.strip() for p in paragraphs for s in p.split('ã€‚') if s.strip()]
            
            # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
            section_patterns = [
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s*(.+)',
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚\s*(.+)',
                r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€\s*(.+)',
                r'^\d+\.\s*(.+)',
                r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]\s*ã€\s*(.+)'
            ]
            
            sections = []
            for paragraph in paragraphs:
                for pattern in section_patterns:
                    match = re.match(pattern, paragraph.strip())
                    if match:
                        sections.append(match.group(1).strip())
                        break
            
            # ç”Ÿæˆç»“æ„æ‘˜è¦
            structure_parts = [
                f"æ–‡æ¡£åŒ…å«{len(paragraphs)}ä¸ªæ®µè½ï¼Œ{len(sentences)}ä¸ªå¥å­"
            ]
            
            if sections:
                structure_parts.append(f"ä¸»è¦ç« èŠ‚ï¼š{len(sections)}ä¸ª")
                if len(sections) <= 5:
                    structure_parts.append(f"ç« èŠ‚æ ‡é¢˜ï¼š{' | '.join(sections)}")
            
            # åˆ†æå†…å®¹ç±»å‹
            content_types = []
            if re.search(r'\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒä¸‡)?å…ƒ', content):
                content_types.append("è´¢åŠ¡æ•°æ®")
            if re.search(r'ç¬¬\d+æ¡|ç¬¬\d+æ¬¾', content):
                content_types.append("æ³•è§„æ¡æ–‡")
            if re.search(r'æµç¨‹|æ­¥éª¤|ç¨‹åº', content):
                content_types.append("æµç¨‹è¯´æ˜")
            if re.search(r'ç›®æ ‡|è®¡åˆ’|è§„åˆ’', content):
                content_types.append("è§„åˆ’ç›®æ ‡")
            
            if content_types:
                structure_parts.append(f"å†…å®¹ç±»å‹ï¼š{' | '.join(content_types)}")
            
            return "ï¼›".join(structure_parts)
            
        except Exception as e:
            logger.error(f"âŒ Structure summary generation failed: {e}")
            return "æ–‡æ¡£ç»“æ„åˆ†æå¤±è´¥"
    
    def _calculate_summary_confidence(self, content: str, brief: str, detailed: str) -> float:
        """è®¡ç®—æ‘˜è¦è´¨é‡ç½®ä¿¡åº¦"""
        try:
            confidence_factors = []
            
            # å†…å®¹é•¿åº¦å› å­
            content_length = len(content)
            if content_length > 1000:
                confidence_factors.append(0.9)
            elif content_length > 500:
                confidence_factors.append(0.7)
            elif content_length > 100:
                confidence_factors.append(0.5)
            else:
                confidence_factors.append(0.3)
            
            # æ‘˜è¦è´¨é‡å› å­
            if brief and len(brief) > 20:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)
            
            if detailed and len(detailed) > 50:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)
            
            # ç»“æ„åŒ–ç¨‹åº¦å› å­
            if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|\d+\.|ç¬¬\d+', content):
                confidence_factors.append(0.9)
            else:
                confidence_factors.append(0.6)
            
            return sum(confidence_factors) / len(confidence_factors)
            
        except Exception as e:
            logger.error(f"âŒ Confidence calculation failed: {e}")
            return 0.5
    
    async def _extract_key_information(self, document_content: Dict[str, Any]) -> List[KeyInformation]:
        """æå–å…³é”®ä¿¡æ¯"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            key_info_list = []
            
            # æŒ‰æ®µè½åˆ†æ
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            for i, paragraph in enumerate(paragraphs):
                # è®¡ç®—æ®µè½é‡è¦æ€§
                importance = self._calculate_paragraph_importance(paragraph)
                
                if importance >= 0.5:  # åªä¿ç•™é‡è¦æ®µè½
                    # ç¡®å®šä¿¡æ¯ç±»åˆ«
                    category = self._classify_information(paragraph)
                    
                    # æå–å…³é”®è¯
                    keywords = self._extract_keywords_from_text(paragraph)
                    
                    # ç¡®å®šæ¥æºç« èŠ‚
                    source_section = f"æ®µè½ {i+1}"
                    
                    key_info = KeyInformation(
                        content=paragraph,
                        importance=importance,
                        category=category,
                        keywords=keywords,
                        source_section=source_section
                    )
                    
                    key_info_list.append(key_info)
            
            # æŒ‰é‡è¦æ€§æ’åºï¼Œè¿”å›å‰10ä¸ª
            key_info_list.sort(key=lambda x: x.importance, reverse=True)
            return key_info_list[:10]
            
        except Exception as e:
            logger.error(f"âŒ Key information extraction failed: {e}")
            return []
    
    def _calculate_paragraph_importance(self, paragraph: str) -> float:
        """è®¡ç®—æ®µè½é‡è¦æ€§"""
        score = 0.0
        
        # é•¿åº¦å› å­
        length = len(paragraph)
        if 100 <= length <= 500:
            score += 0.3
        elif 50 <= length <= 100 or 500 <= length <= 1000:
            score += 0.2
        
        # å…³é”®è¯å› å­
        important_words = [
            "é‡è¦", "å…³é”®", "æ ¸å¿ƒ", "ä¸»è¦", "åŸºæœ¬", "æ ¹æœ¬", "é¦–è¦", "å¿…é¡»", "åº”å½“",
            "ç›®æ ‡", "åŸåˆ™", "è¦æ±‚", "è§„å®š", "æ ‡å‡†", "åˆ¶åº¦", "æ”¿ç­–", "æ³•è§„",
            "é£é™©", "åˆè§„", "æ²»ç†", "ç®¡ç†", "æ§åˆ¶", "ç›‘ç£", "å®¡è®¡",
            "ESG", "ç¯å¢ƒ", "ç¤¾ä¼š", "è´£ä»»", "å¯æŒç»­", "å‘å±•"
        ]
        
        word_count = sum(1 for word in important_words if word in paragraph)
        score += min(word_count * 0.1, 0.4)
        
        # æ•°æ®å› å­
        if re.search(r'\d+(?:\.\d+)?%', paragraph):
            score += 0.1
        if re.search(r'\d+(?:\.\d+)?(?:ä¸‡|äº¿|åƒä¸‡)?å…ƒ', paragraph):
            score += 0.1
        
        # ç»“æ„å› å­
        if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€|\d+\.|ç¬¬\d+', paragraph):
            score += 0.1
        
        return min(score, 1.0)
    
    def _classify_information(self, text: str) -> str:
        """å¯¹ä¿¡æ¯è¿›è¡Œåˆ†ç±»"""
        for category, keywords in self.information_categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        return "å…¶ä»–ä¿¡æ¯"
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå– - åŸºäºè¯é¢‘å’Œé‡è¦æ€§
        words = re.findall(r'[\u4e00-\u9fff]+', text)  # æå–ä¸­æ–‡è¯æ±‡
        
        # è¿‡æ»¤åœç”¨è¯
        stop_words = {
            "çš„", "äº†", "åœ¨", "æ˜¯", "æˆ‘", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "éƒ½", "ä¸€", "ä¸€ä¸ª",
            "ä¸Š", "ä¹Ÿ", "å¾ˆ", "åˆ°", "è¯´", "è¦", "å»", "ä½ ", "ä¼š", "ç€", "æ²¡æœ‰", "çœ‹", "å¥½",
            "è‡ªå·±", "è¿™", "é‚£", "å®ƒ", "ä»¬", "è¿™ä¸ª", "é‚£ä¸ª", "ä»€ä¹ˆ", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ"
        }
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            if len(word) >= 2 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œè¿”å›å‰5ä¸ª
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:5]]
    
    async def _extract_entities(self, document_content: Dict[str, Any]) -> List[ExtractedEntity]:
        """æå–å®ä½“ä¿¡æ¯"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            entities = []
            
            for entity_type, patterns in self.entity_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    
                    for match in matches:
                        entity_text = match.group(0)
                        start_pos = match.start()
                        
                        # è·å–ä¸Šä¸‹æ–‡
                        context_start = max(0, start_pos - 50)
                        context_end = min(len(content), start_pos + len(entity_text) + 50)
                        context = content[context_start:context_end]
                        
                        # è®¡ç®—ç½®ä¿¡åº¦
                        confidence = self._calculate_entity_confidence(entity_text, entity_type, context)
                        
                        if confidence >= 0.5:  # åªä¿ç•™é«˜ç½®ä¿¡åº¦çš„å®ä½“
                            entity = ExtractedEntity(
                                text=entity_text,
                                type=entity_type,
                                confidence=confidence,
                                context=context,
                                position=start_pos
                            )
                            entities.append(entity)
            
            # å»é‡å¹¶æŒ‰ç½®ä¿¡åº¦æ’åº
            unique_entities = self._deduplicate_entities(entities)
            unique_entities.sort(key=lambda x: x.confidence, reverse=True)
            
            return unique_entities[:20]  # è¿”å›å‰20ä¸ªå®ä½“
            
        except Exception as e:
            logger.error(f"âŒ Entity extraction failed: {e}")
            return []
    
    def _calculate_entity_confidence(self, entity_text: str, entity_type: str, context: str) -> float:
        """è®¡ç®—å®ä½“è¯†åˆ«ç½®ä¿¡åº¦"""
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦
        
        # é•¿åº¦å› å­
        if 2 <= len(entity_text) <= 20:
            confidence += 0.2
        
        # ç±»å‹ç‰¹å®šçš„ç½®ä¿¡åº¦è°ƒæ•´
        type_adjustments = {
            "ç»„ç»‡æœºæ„": 0.1 if "å…¬å¸" in entity_text or "ä¼ä¸š" in entity_text else 0,
            "è´¢åŠ¡æ•°æ®": 0.2 if re.search(r'\d', entity_text) else 0,
            "æ—¶é—´æ—¥æœŸ": 0.2 if re.search(r'\d{4}', entity_text) else 0,
            "æ”¿ç­–æ³•è§„": 0.1 if "æ³•" in entity_text or "è§„" in entity_text else 0
        }
        
        confidence += type_adjustments.get(entity_type, 0)
        
        # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        if entity_type in context:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _deduplicate_entities(self, entities: List[ExtractedEntity]) -> List[ExtractedEntity]:
        """å»é™¤é‡å¤å®ä½“"""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            # ä½¿ç”¨æ–‡æœ¬å’Œç±»å‹ä½œä¸ºå»é‡æ ‡è¯†
            key = (entity.text.lower(), entity.type)
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    async def _generate_tags(self, document_content: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆæ–‡æ¡£æ ‡ç­¾"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            tags = set()
            
            # åŸºäºå®ä½“ç±»å‹ç”Ÿæˆæ ‡ç­¾
            for entity_type, patterns in self.entity_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        tags.add(entity_type)
            
            # åŸºäºä¿¡æ¯ç±»åˆ«ç”Ÿæˆæ ‡ç­¾
            for category, keywords in self.information_categories.items():
                if any(keyword in content for keyword in keywords):
                    tags.add(category)
            
            # åŸºäºæ–‡æ¡£ç‰¹å¾ç”Ÿæˆæ ‡ç­¾
            if re.search(r'æŠ¥å‘Š|å¹´æŠ¥|å­£æŠ¥', content):
                tags.add("æŠ¥å‘Šæ–‡æ¡£")
            if re.search(r'åˆ¶åº¦|è§„å®š|æµç¨‹|ç¨‹åº', content):
                tags.add("åˆ¶åº¦æ–‡æ¡£")
            if re.search(r'åˆåŒ|åè®®|æ¡æ¬¾', content):
                tags.add("åˆåŒæ–‡æ¡£")
            if re.search(r'è®¡åˆ’|æ–¹æ¡ˆ|ç­–ç•¥', content):
                tags.add("è§„åˆ’æ–‡æ¡£")
            
            # åŸºäºæ–‡æ¡£é•¿åº¦ç”Ÿæˆæ ‡ç­¾
            word_count = len(content)
            if word_count > 5000:
                tags.add("é•¿ç¯‡æ–‡æ¡£")
            elif word_count > 1000:
                tags.add("ä¸­ç¯‡æ–‡æ¡£")
            else:
                tags.add("çŸ­ç¯‡æ–‡æ¡£")
            
            return list(tags)[:10]  # é™åˆ¶æ ‡ç­¾æ•°é‡
            
        except Exception as e:
            logger.error(f"âŒ Tag generation failed: {e}")
            return []
    
    async def _analyze_document_structure(self, document_content: Dict[str, Any]) -> Dict[str, int]:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        try:
            content = document_content.get('content', '')
            if not content:
                return {"word_count": 0, "paragraph_count": 0, "section_count": 0}
            
            # å­—æ•°ç»Ÿè®¡
            word_count = len(re.findall(r'[\u4e00-\u9fff]', content))  # ä¸­æ–‡å­—ç¬¦æ•°
            word_count += len(re.findall(r'[a-zA-Z]+', content))  # è‹±æ–‡å•è¯æ•°
            
            # æ®µè½ç»Ÿè®¡
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            paragraph_count = len(paragraphs)
            
            # ç« èŠ‚ç»Ÿè®¡
            section_patterns = [
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ',
                r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+èŠ‚',
                r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€',
                r'^\d+\.',
            ]
            
            section_count = 0
            for paragraph in paragraphs:
                for pattern in section_patterns:
                    if re.match(pattern, paragraph.strip()):
                        section_count += 1
                        break
            
            return {
                "word_count": word_count,
                "paragraph_count": paragraph_count,
                "section_count": section_count
            }
            
        except Exception as e:
            logger.error(f"âŒ Document structure analysis failed: {e}")
            return {"word_count": 0, "paragraph_count": 0, "section_count": 0}


# å…¨å±€ä¿¡æ¯æå–æœåŠ¡å®ä¾‹
_extraction_service_instance = None


def get_extraction_service() -> InformationExtractionService:
    """è·å–ä¿¡æ¯æå–æœåŠ¡å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _extraction_service_instance
    if _extraction_service_instance is None:
        _extraction_service_instance = InformationExtractionService()
    return _extraction_service_instance 