"""
信息提取与摘要生成服务
支持文档关键信息提取、实体识别、标签生成和多层次摘要生成
"""

import logging
import re
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
from dataclasses import dataclass, asdict
import asyncio

from app.services.knowledge_service import get_knowledge_service
from app.vector_store.chroma_db import get_chroma_manager
from app.core.config import settings

logger = logging.getLogger(__name__)


@dataclass
class ExtractedEntity:
    """提取的实体信息"""
    text: str  # 实体文本
    type: str  # 实体类型
    confidence: float  # 置信度
    context: str  # 上下文
    position: int  # 在文档中的位置


@dataclass 
class KeyInformation:
    """关键信息点"""
    content: str  # 信息内容
    importance: float  # 重要性评分
    category: str  # 信息类别
    keywords: List[str]  # 关键词
    source_section: str  # 来源章节


@dataclass
class DocumentSummary:
    """文档摘要"""
    title: str  # 文档标题
    brief_summary: str  # 简要摘要（1-2句话）
    detailed_summary: str  # 详细摘要（段落级别）
    key_points: List[str]  # 关键要点
    structure_summary: str  # 结构性摘要
    confidence: float  # 摘要质量置信度


@dataclass
class ExtractionResult:
    """信息提取结果"""
    document_id: str
    document_name: str
    
    # 基础信息
    summary: DocumentSummary
    key_information: List[KeyInformation]
    entities: List[ExtractedEntity]
    tags: List[str]
    
    # 统计信息
    word_count: int
    paragraph_count: int
    section_count: int
    
    # 元数据
    extraction_timestamp: datetime
    processing_time: float
    
    def to_dict(self) -> Dict[str, Any]:
        """转换为字典格式"""
        return asdict(self)


class InformationExtractionService:
    """
    信息提取与摘要生成服务
    
    核心功能：
    1. 关键信息提取
    2. 实体识别与分类
    3. 自动标签生成
    4. 多层次摘要生成
    5. 结构化信息输出
    """
    
    def __init__(self):
        """初始化信息提取服务"""
        self.knowledge_service = None
        self.chroma_manager = None
        
        # 预定义的实体类型和模式
        self.entity_patterns = {
            "组织机构": [
                r"公司|企业|集团|有限公司|股份公司|机构|部门|委员会",
                r"[A-Z][a-zA-Z\s]+(?:公司|Company|Corp|Inc|Ltd)"
            ],
            "人员职位": [
                r"总经理|董事长|CEO|CTO|CFO|总监|经理|主管|负责人|专员",
                r"董事|监事|股东|投资者|员工|管理层"
            ],
            "政策法规": [
                r"法律|法规|条例|规定|制度|政策|标准|规范|准则|指导意见",
                r"《[^》]+》|第\d+条|第\d+款|第\d+项"
            ],
            "财务数据": [
                r"\d+(?:\.\d+)?(?:万|亿|千万)?元",
                r"\d+(?:\.\d+)?%",
                r"营收|利润|成本|费用|资产|负债|现金流"
            ],
            "时间日期": [
                r"\d{4}年(?:\d{1,2}月)?(?:\d{1,2}日)?",
                r"\d{1,2}月\d{1,2}日",
                r"第[一二三四]季度|上半年|下半年|年度"
            ],
            "ESG指标": [
                r"碳排放|节能减排|环境保护|可持续发展|社会责任",
                r"公司治理|合规|风险管理|内控|审计"
            ]
        }
        
        # 关键信息类别
        self.information_categories = {
            "核心业务": ["业务", "产品", "服务", "市场", "客户"],
            "财务状况": ["财务", "收入", "利润", "成本", "投资"],
            "风险管理": ["风险", "合规", "内控", "审计", "监管"],
            "治理结构": ["治理", "董事会", "管理层", "股东", "决策"],
            "ESG表现": ["环境", "社会", "治理", "可持续", "责任"],
            "战略规划": ["战略", "规划", "目标", "发展", "创新"]
        }
        
        logger.info("🧠 Information Extraction Service initialized")
    
    async def _init_components(self):
        """异步初始化组件"""
        if not self.knowledge_service:
            self.knowledge_service = get_knowledge_service()
        if not self.chroma_manager:
            self.chroma_manager = get_chroma_manager()
    
    async def extract_information(self, document_id: str, user_id: str) -> ExtractionResult:
        """
        提取文档的关键信息
        
        Args:
            document_id: 文档ID
            user_id: 用户ID
            
        Returns:
            ExtractionResult: 提取结果
        """
        start_time = datetime.now()
        
        try:
            await self._init_components()
            
            logger.info(f"🔍 Starting information extraction for document: {document_id}")
            
            # 1. 获取文档内容
            document_content = await self._get_document_content(document_id, user_id)
            if not document_content:
                raise ValueError(f"Document {document_id} not found or empty")
            
            # 2. 并行执行各种提取任务
            extraction_tasks = [
                self._extract_summary(document_content),
                self._extract_key_information(document_content),
                self._extract_entities(document_content),
                self._generate_tags(document_content),
                self._analyze_document_structure(document_content)
            ]
            
            results = await asyncio.gather(*extraction_tasks)
            summary, key_info, entities, tags, structure_stats = results
            
            # 3. 构建提取结果
            processing_time = (datetime.now() - start_time).total_seconds()
            
            extraction_result = ExtractionResult(
                document_id=document_id,
                document_name=document_content.get('name', 'Unknown Document'),
                summary=summary,
                key_information=key_info,
                entities=entities,
                tags=tags,
                word_count=structure_stats['word_count'],
                paragraph_count=structure_stats['paragraph_count'],
                section_count=structure_stats['section_count'],
                extraction_timestamp=datetime.now(),
                processing_time=processing_time
            )
            
            logger.info(f"✅ Information extraction completed in {processing_time:.2f}s")
            return extraction_result
            
        except Exception as e:
            logger.error(f"❌ Information extraction failed: {e}")
            raise
    
    async def _get_document_content(self, document_id: str, user_id: str) -> Dict[str, Any]:
        """获取文档内容"""
        try:
            # 从向量数据库获取文档片段
            search_results = self.chroma_manager.collection.query(
                query_texts=[""],  # 空查询获取所有片段
                n_results=100,  # 获取更多片段
                where={
                    "document_id": document_id,
                    "user_id": user_id
                }
            )
            
            if not search_results.get('documents') or not search_results['documents'][0]:
                return {}
            
            # 合并所有文档片段
            documents = search_results['documents'][0]
            metadatas = search_results.get('metadatas', [[]])[0]
            
            # 按chunk_index排序（如果有的话）
            combined_docs = list(zip(documents, metadatas))
            combined_docs.sort(key=lambda x: x[1].get('chunk_index', 0))
            
            full_content = "\n\n".join([doc for doc, _ in combined_docs])
            
            # 获取文档元信息
            document_name = metadatas[0].get('filename', 'Unknown Document') if metadatas else 'Unknown Document'
            
            return {
                'content': full_content,
                'name': document_name,
                'metadata': metadatas[0] if metadatas else {}
            }
            
        except Exception as e:
            logger.error(f"❌ Failed to get document content: {e}")
            return {}
    
    async def _extract_summary(self, document_content: Dict[str, Any]) -> DocumentSummary:
        """生成文档摘要"""
        try:
            content = document_content.get('content', '')
            doc_name = document_content.get('name', 'Unknown Document')
            
            if not content:
                return DocumentSummary(
                    title=doc_name,
                    brief_summary="文档内容为空",
                    detailed_summary="无法生成摘要，文档内容为空。",
                    key_points=[],
                    structure_summary="文档结构：无内容",
                    confidence=0.0
                )
            
            # 分析文档结构
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            sentences = [s.strip() for p in paragraphs for s in p.split('。') if s.strip()]
            
            # 简要摘要 - 取前几个重要句子
            brief_sentences = sentences[:2] if len(sentences) >= 2 else sentences
            brief_summary = '。'.join(brief_sentences)
            if brief_summary and not brief_summary.endswith('。'):
                brief_summary += '。'
            
            # 详细摘要 - 基于段落生成
            important_paragraphs = self._select_important_paragraphs(paragraphs)
            detailed_summary = self._generate_detailed_summary(important_paragraphs)
            
            # 关键要点提取
            key_points = self._extract_key_points(content)
            
            # 结构性摘要
            structure_summary = self._generate_structure_summary(content)
            
            # 计算置信度
            confidence = self._calculate_summary_confidence(content, brief_summary, detailed_summary)
            
            return DocumentSummary(
                title=doc_name,
                brief_summary=brief_summary or "文档概述信息不足",
                detailed_summary=detailed_summary,
                key_points=key_points,
                structure_summary=structure_summary,
                confidence=confidence
            )
            
        except Exception as e:
            logger.error(f"❌ Summary extraction failed: {e}")
            return DocumentSummary(
                title=document_content.get('name', 'Unknown Document'),
                brief_summary="摘要生成失败",
                detailed_summary="由于技术原因，无法生成文档摘要。",
                key_points=[],
                structure_summary="结构分析失败",
                confidence=0.0
            )
    
    def _select_important_paragraphs(self, paragraphs: List[str], max_paragraphs: int = 5) -> List[str]:
        """选择重要段落"""
        if not paragraphs:
            return []
        
        # 段落重要性评分
        paragraph_scores = []
        
        for paragraph in paragraphs:
            score = 0
            
            # 长度评分（适中长度更重要）
            length = len(paragraph)
            if 50 <= length <= 500:
                score += 2
            elif 20 <= length <= 50 or 500 <= length <= 1000:
                score += 1
            
            # 关键词评分
            important_keywords = [
                "重要", "关键", "核心", "主要", "基本", "根本", "首要",
                "目标", "原则", "要求", "规定", "标准", "流程", "制度",
                "总结", "结论", "建议", "措施", "方案", "计划"
            ]
            
            for keyword in important_keywords:
                if keyword in paragraph:
                    score += 1
            
            # 数字和百分比（通常包含重要信息）
            if re.search(r'\d+(?:\.\d+)?%', paragraph):
                score += 1
            if re.search(r'\d+(?:\.\d+)?(?:万|亿|千万)?元', paragraph):
                score += 1
            
            # 列表或条目（通常是要点）
            if re.search(r'[一二三四五六七八九十]、|[1-9]\.|[1-9]）', paragraph):
                score += 1
            
            paragraph_scores.append((paragraph, score))
        
        # 按评分排序并选择前N个
        paragraph_scores.sort(key=lambda x: x[1], reverse=True)
        selected = [p for p, _ in paragraph_scores[:max_paragraphs]]
        
        return selected
    
    def _generate_detailed_summary(self, important_paragraphs: List[str]) -> str:
        """生成详细摘要"""
        if not important_paragraphs:
            return "文档内容分析不足，无法生成详细摘要。"
        
        # 简化版摘要生成 - 提取每个段落的核心句子
        summary_parts = []
        
        for paragraph in important_paragraphs:
            # 取段落的第一句或最重要的句子
            sentences = [s.strip() for s in paragraph.split('。') if s.strip()]
            if sentences:
                # 选择最长的句子作为代表（通常包含更多信息）
                representative_sentence = max(sentences, key=len)
                if len(representative_sentence) > 10:  # 过滤过短的句子
                    summary_parts.append(representative_sentence)
        
        # 组合摘要
        if summary_parts:
            detailed_summary = '。'.join(summary_parts[:3])  # 最多3句
            if not detailed_summary.endswith('。'):
                detailed_summary += '。'
            return detailed_summary
        else:
            return "基于文档内容分析，未能提取到足够的关键信息生成详细摘要。"
    
    def _extract_key_points(self, content: str) -> List[str]:
        """提取关键要点"""
        key_points = []
        
        # 查找明显的要点标记
        patterns = [
            r'[一二三四五六七八九十]、([^。]+)',
            r'[1-9]\.\s*([^。]+)',
            r'[1-9]）\s*([^。]+)',
            r'•\s*([^。]+)',
            r'◆\s*([^。]+)',
            r'★\s*([^。]+)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                point = match.strip()
                if len(point) > 5 and len(point) < 200:  # 过滤过短或过长的内容
                    key_points.append(point)
        
        # 如果没有找到明显的要点，从重要句子中提取
        if not key_points:
            sentences = [s.strip() for s in content.split('。') if s.strip()]
            
            # 查找包含关键词的句子
            important_keywords = ["重要", "关键", "核心", "主要", "必须", "应当", "需要"]
            
            for sentence in sentences:
                if len(sentence) > 10 and len(sentence) < 150:
                    for keyword in important_keywords:
                        if keyword in sentence:
                            key_points.append(sentence)
                            break
                
                if len(key_points) >= 5:  # 限制数量
                    break
        
        return key_points[:5]  # 返回前5个要点
    
    def _generate_structure_summary(self, content: str) -> str:
        """生成结构性摘要"""
        try:
            # 分析文档结构
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            sentences = [s.strip() for p in paragraphs for s in p.split('。') if s.strip()]
            
            # 查找章节标题
            section_patterns = [
                r'^第[一二三四五六七八九十\d]+章\s*(.+)',
                r'^第[一二三四五六七八九十\d]+节\s*(.+)',
                r'^[一二三四五六七八九十]、\s*(.+)',
                r'^\d+\.\s*(.+)',
                r'^[一二三四五六七八九十]\s*、\s*(.+)'
            ]
            
            sections = []
            for paragraph in paragraphs:
                for pattern in section_patterns:
                    match = re.match(pattern, paragraph.strip())
                    if match:
                        sections.append(match.group(1).strip())
                        break
            
            # 生成结构摘要
            structure_parts = [
                f"文档包含{len(paragraphs)}个段落，{len(sentences)}个句子"
            ]
            
            if sections:
                structure_parts.append(f"主要章节：{len(sections)}个")
                if len(sections) <= 5:
                    structure_parts.append(f"章节标题：{' | '.join(sections)}")
            
            # 分析内容类型
            content_types = []
            if re.search(r'\d+(?:\.\d+)?(?:万|亿|千万)?元', content):
                content_types.append("财务数据")
            if re.search(r'第\d+条|第\d+款', content):
                content_types.append("法规条文")
            if re.search(r'流程|步骤|程序', content):
                content_types.append("流程说明")
            if re.search(r'目标|计划|规划', content):
                content_types.append("规划目标")
            
            if content_types:
                structure_parts.append(f"内容类型：{' | '.join(content_types)}")
            
            return "；".join(structure_parts)
            
        except Exception as e:
            logger.error(f"❌ Structure summary generation failed: {e}")
            return "文档结构分析失败"
    
    def _calculate_summary_confidence(self, content: str, brief: str, detailed: str) -> float:
        """计算摘要质量置信度"""
        try:
            confidence_factors = []
            
            # 内容长度因子
            content_length = len(content)
            if content_length > 1000:
                confidence_factors.append(0.9)
            elif content_length > 500:
                confidence_factors.append(0.7)
            elif content_length > 100:
                confidence_factors.append(0.5)
            else:
                confidence_factors.append(0.3)
            
            # 摘要质量因子
            if brief and len(brief) > 20:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)
            
            if detailed and len(detailed) > 50:
                confidence_factors.append(0.8)
            else:
                confidence_factors.append(0.4)
            
            # 结构化程度因子
            if re.search(r'[一二三四五六七八九十]、|\d+\.|第\d+', content):
                confidence_factors.append(0.9)
            else:
                confidence_factors.append(0.6)
            
            return sum(confidence_factors) / len(confidence_factors)
            
        except Exception as e:
            logger.error(f"❌ Confidence calculation failed: {e}")
            return 0.5
    
    async def _extract_key_information(self, document_content: Dict[str, Any]) -> List[KeyInformation]:
        """提取关键信息"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            key_info_list = []
            
            # 按段落分析
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            for i, paragraph in enumerate(paragraphs):
                # 计算段落重要性
                importance = self._calculate_paragraph_importance(paragraph)
                
                if importance >= 0.5:  # 只保留重要段落
                    # 确定信息类别
                    category = self._classify_information(paragraph)
                    
                    # 提取关键词
                    keywords = self._extract_keywords_from_text(paragraph)
                    
                    # 确定来源章节
                    source_section = f"段落 {i+1}"
                    
                    key_info = KeyInformation(
                        content=paragraph,
                        importance=importance,
                        category=category,
                        keywords=keywords,
                        source_section=source_section
                    )
                    
                    key_info_list.append(key_info)
            
            # 按重要性排序，返回前10个
            key_info_list.sort(key=lambda x: x.importance, reverse=True)
            return key_info_list[:10]
            
        except Exception as e:
            logger.error(f"❌ Key information extraction failed: {e}")
            return []
    
    def _calculate_paragraph_importance(self, paragraph: str) -> float:
        """计算段落重要性"""
        score = 0.0
        
        # 长度因子
        length = len(paragraph)
        if 100 <= length <= 500:
            score += 0.3
        elif 50 <= length <= 100 or 500 <= length <= 1000:
            score += 0.2
        
        # 关键词因子
        important_words = [
            "重要", "关键", "核心", "主要", "基本", "根本", "首要", "必须", "应当",
            "目标", "原则", "要求", "规定", "标准", "制度", "政策", "法规",
            "风险", "合规", "治理", "管理", "控制", "监督", "审计",
            "ESG", "环境", "社会", "责任", "可持续", "发展"
        ]
        
        word_count = sum(1 for word in important_words if word in paragraph)
        score += min(word_count * 0.1, 0.4)
        
        # 数据因子
        if re.search(r'\d+(?:\.\d+)?%', paragraph):
            score += 0.1
        if re.search(r'\d+(?:\.\d+)?(?:万|亿|千万)?元', paragraph):
            score += 0.1
        
        # 结构因子
        if re.search(r'[一二三四五六七八九十]、|\d+\.|第\d+', paragraph):
            score += 0.1
        
        return min(score, 1.0)
    
    def _classify_information(self, text: str) -> str:
        """对信息进行分类"""
        for category, keywords in self.information_categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        return "其他信息"
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """从文本中提取关键词"""
        # 简单的关键词提取 - 基于词频和重要性
        words = re.findall(r'[\u4e00-\u9fff]+', text)  # 提取中文词汇
        
        # 过滤停用词
        stop_words = {
            "的", "了", "在", "是", "我", "有", "和", "就", "不", "人", "都", "一", "一个",
            "上", "也", "很", "到", "说", "要", "去", "你", "会", "着", "没有", "看", "好",
            "自己", "这", "那", "它", "们", "这个", "那个", "什么", "怎么", "为什么"
        }
        
        # 统计词频
        word_freq = {}
        for word in words:
            if len(word) >= 2 and word not in stop_words:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # 按频率排序，返回前5个
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:5]]
    
    async def _extract_entities(self, document_content: Dict[str, Any]) -> List[ExtractedEntity]:
        """提取实体信息"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            entities = []
            
            for entity_type, patterns in self.entity_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, content, re.IGNORECASE)
                    
                    for match in matches:
                        entity_text = match.group(0)
                        start_pos = match.start()
                        
                        # 获取上下文
                        context_start = max(0, start_pos - 50)
                        context_end = min(len(content), start_pos + len(entity_text) + 50)
                        context = content[context_start:context_end]
                        
                        # 计算置信度
                        confidence = self._calculate_entity_confidence(entity_text, entity_type, context)
                        
                        if confidence >= 0.5:  # 只保留高置信度的实体
                            entity = ExtractedEntity(
                                text=entity_text,
                                type=entity_type,
                                confidence=confidence,
                                context=context,
                                position=start_pos
                            )
                            entities.append(entity)
            
            # 去重并按置信度排序
            unique_entities = self._deduplicate_entities(entities)
            unique_entities.sort(key=lambda x: x.confidence, reverse=True)
            
            return unique_entities[:20]  # 返回前20个实体
            
        except Exception as e:
            logger.error(f"❌ Entity extraction failed: {e}")
            return []
    
    def _calculate_entity_confidence(self, entity_text: str, entity_type: str, context: str) -> float:
        """计算实体识别置信度"""
        confidence = 0.5  # 基础置信度
        
        # 长度因子
        if 2 <= len(entity_text) <= 20:
            confidence += 0.2
        
        # 类型特定的置信度调整
        type_adjustments = {
            "组织机构": 0.1 if "公司" in entity_text or "企业" in entity_text else 0,
            "财务数据": 0.2 if re.search(r'\d', entity_text) else 0,
            "时间日期": 0.2 if re.search(r'\d{4}', entity_text) else 0,
            "政策法规": 0.1 if "法" in entity_text or "规" in entity_text else 0
        }
        
        confidence += type_adjustments.get(entity_type, 0)
        
        # 上下文相关性
        if entity_type in context:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def _deduplicate_entities(self, entities: List[ExtractedEntity]) -> List[ExtractedEntity]:
        """去除重复实体"""
        seen = set()
        unique_entities = []
        
        for entity in entities:
            # 使用文本和类型作为去重标识
            key = (entity.text.lower(), entity.type)
            if key not in seen:
                seen.add(key)
                unique_entities.append(entity)
        
        return unique_entities
    
    async def _generate_tags(self, document_content: Dict[str, Any]) -> List[str]:
        """生成文档标签"""
        try:
            content = document_content.get('content', '')
            if not content:
                return []
            
            tags = set()
            
            # 基于实体类型生成标签
            for entity_type, patterns in self.entity_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        tags.add(entity_type)
            
            # 基于信息类别生成标签
            for category, keywords in self.information_categories.items():
                if any(keyword in content for keyword in keywords):
                    tags.add(category)
            
            # 基于文档特征生成标签
            if re.search(r'报告|年报|季报', content):
                tags.add("报告文档")
            if re.search(r'制度|规定|流程|程序', content):
                tags.add("制度文档")
            if re.search(r'合同|协议|条款', content):
                tags.add("合同文档")
            if re.search(r'计划|方案|策略', content):
                tags.add("规划文档")
            
            # 基于文档长度生成标签
            word_count = len(content)
            if word_count > 5000:
                tags.add("长篇文档")
            elif word_count > 1000:
                tags.add("中篇文档")
            else:
                tags.add("短篇文档")
            
            return list(tags)[:10]  # 限制标签数量
            
        except Exception as e:
            logger.error(f"❌ Tag generation failed: {e}")
            return []
    
    async def _analyze_document_structure(self, document_content: Dict[str, Any]) -> Dict[str, int]:
        """分析文档结构"""
        try:
            content = document_content.get('content', '')
            if not content:
                return {"word_count": 0, "paragraph_count": 0, "section_count": 0}
            
            # 字数统计
            word_count = len(re.findall(r'[\u4e00-\u9fff]', content))  # 中文字符数
            word_count += len(re.findall(r'[a-zA-Z]+', content))  # 英文单词数
            
            # 段落统计
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            paragraph_count = len(paragraphs)
            
            # 章节统计
            section_patterns = [
                r'^第[一二三四五六七八九十\d]+章',
                r'^第[一二三四五六七八九十\d]+节',
                r'^[一二三四五六七八九十]、',
                r'^\d+\.',
            ]
            
            section_count = 0
            for paragraph in paragraphs:
                for pattern in section_patterns:
                    if re.match(pattern, paragraph.strip()):
                        section_count += 1
                        break
            
            return {
                "word_count": word_count,
                "paragraph_count": paragraph_count,
                "section_count": section_count
            }
            
        except Exception as e:
            logger.error(f"❌ Document structure analysis failed: {e}")
            return {"word_count": 0, "paragraph_count": 0, "section_count": 0}


# 全局信息提取服务实例
_extraction_service_instance = None


def get_extraction_service() -> InformationExtractionService:
    """获取信息提取服务实例（单例模式）"""
    global _extraction_service_instance
    if _extraction_service_instance is None:
        _extraction_service_instance = InformationExtractionService()
    return _extraction_service_instance 